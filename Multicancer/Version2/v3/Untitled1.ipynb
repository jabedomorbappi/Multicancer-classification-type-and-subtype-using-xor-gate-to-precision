{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMciENO2XNvwffqLZiX15ms"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"k3k5ljiEb397","colab":{"base_uri":"https://localhost:8080/","height":391},"executionInfo":{"status":"error","timestamp":1702746398736,"user_tz":-360,"elapsed":51966,"user":{"displayName":"jabed omor","userId":"16128518018356494995"}},"outputId":"5ebd7845-9f15-4856-b471-15df5adc9890"},"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-5fed961b4776>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    127\u001b[0m   )\n\u001b[1;32m    128\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import os\n","\n","import json\n","import os\n","\n","# Specify the path to your Kaggle API credentials JSON file\n","kaggle_json_path = '/content/kaggle(2).json'  # Update with the actual path\n","\n","# Check if the JSON file exists\n","if os.path.exists(kaggle_json_path):\n","    with open(kaggle_json_path, 'r') as json_file:\n","        kaggle_credentials = json.load(json_file)\n","\n","    # Extract username and key from the JSON file\n","    kaggle_username = kaggle_credentials['username']\n","    kaggle_key = kaggle_credentials['key']\n","\n","    # Set Kaggle API credentials as environment variables\n","    os.environ['KAGGLE_USERNAME'] = kaggle_username\n","    os.environ['KAGGLE_KEY'] = kaggle_key\n","else:\n","    print(\"Kaggle API credentials JSON file not found.\")\n","\n","\n","import os\n","os.environ['KAGGLE_CONFIG_DIR'] = '/content/'\n","!kaggle datasets download -d mahibuzzaman/multicancer-all-classes7-cancer-23classes224x224 -p '/content/'\n","\n","\n","import zipfile\n","\n","with zipfile.ZipFile('/content/multicancer-all-classes7-cancer-23classes224x224.zip', 'r') as zip_ref:\n","    zip_ref.extractall('/content/')"]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import cv2\n","import tensorflow as tf\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.layers import Input, Flatten, Dense\n","from tensorflow.keras.applications import VGG16\n","from tensorflow.keras.models import Model\n","\n","import numpy as np\n","import pandas as pd\n","\n","import matplotlib.pyplot as plt\n","import os\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import cv2\n","import tensorflow as tf\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.layers import Input, Flatten, Dense\n","from tensorflow.keras.applications import VGG16\n","from tensorflow.keras.models import Model\n","import random\n","\n","\n","\n","\n","\n","import numpy as np\n","import pandas as pd\n","import cv2\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tqdm import tqdm\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf\n","from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout,AveragePooling2D\n","from tensorflow.keras.models import Model\n","\n","\n","from tensorflow.keras import Sequential\n","\n","from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras.layers import BatchNormalization, Input, Conv2D, MaxPooling2D, Flatten, Dense\n","\n","# Define the common convolutional layers\n","from tensorflow.keras.layers import BatchNormalization, Input, Conv2D, MaxPooling2D, Flatten, Dense\n","from tensorflow.keras.layers import BatchNormalization, Dropout, Flatten, Dense\n","\n","from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout\n","from tensorflow.keras.models import Model\n","import numpy as np\n","import pandas as pd\n","import cv2\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tqdm import tqdm\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.layers import MaxPooling2D\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, MaxPooling2D, Flatten, Dense, Dropout\n","from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n","from keras.layers import Dense,Conv2D,Flatten,MaxPool2D,Dropout,BatchNormalization, Input\n","from keras.optimizers import Adam\n","from keras.callbacks import ReduceLROnPlateau\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","import PIL.Image as Image, PIL.ImageDraw as ImageDraw, PIL.ImageFont as ImageFont\n","from matplotlib import pyplot as plt\n","import seaborn as sns\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator"],"metadata":{"id":"awYk1Ykvb_Tv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Assuming 'main_class' and 'sub_class' are string columns, convert them to a categorical type\n","train_df['main_class'] = train_df['main_class'].astype('category')\n","train_df['sub_class'] = train_df['sub_class'].astype('category')\n","\n","val_df['main_class'] = val_df['main_class'].astype('category')\n","val_df['sub_class'] = val_df['sub_class'].astype('category')\n","\n","test_df['main_class'] = test_df['main_class'].astype('category')\n","test_df['sub_class'] = test_df['sub_class'].astype('category')\n","\n","IMG_SIZE = 200\n","\n","# Load the pre-trained VGG16 model with weights trained on ImageNet data\n","base_model = VGG16(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n","\n","# Freeze the layers of the pre-trained model\n","for layer in base_model.layers:\n","    layer.trainable = False\n","\n","# Create a new input layer for your custom classification layers\n","inputs = Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n","\n","# Connect your input to the base VGG16 model\n","x = base_model(inputs)\n","x = Flatten()(x)\n","x = Dense(256, activation='relu')(x)\n","\n","# Main class classification\n","main_class_output = Dense(train_df['main_class'].nunique(), activation='softmax', name='output_main')(x)\n","\n","# Create a model for main class classification\n","model_main = Model(inputs=inputs, outputs=[main_class_output])\n","\n","model_main.compile(optimizer=Adam(lr=0.0001),\n","                   loss={'output_main': 'categorical_crossentropy'},\n","                   metrics={'output_main': 'accuracy'})\n","\n","# Create data generator for main_class prediction\n","datagen_main = ImageDataGenerator(rescale=1.0 / 255.0)\n","\n","output_block1_batch_size = 512\n","\n","output_block1_train_generator = datagen_main.flow_from_dataframe(\n","    dataframe=train_df,\n","    x_col='image_path',\n","    y_col='main_class',\n","    class_mode='categorical',\n","    target_size=(IMG_SIZE, IMG_SIZE),\n","    batch_size=output_block1_batch_size\n",")\n","\n","output_block1_val_generator = datagen_main.flow_from_dataframe(\n","    dataframe=val_df,\n","    x_col='image_path',\n","    y_col='main_class',\n","    class_mode='categorical',\n","    target_size=(IMG_SIZE, IMG_SIZE),\n","    batch_size=output_block1_batch_size\n",")\n","\n","# Train the main_class prediction model\n","model_main.fit(\n","    output_block1_train_generator,\n","    epochs=10,  # Adjust the number of epochs\n","    validation_data=output_block1_val_generator\n",")\n","\n","# Predict main classes for the test dataset\n","output_block1_test_generator = datagen_main.flow_from_dataframe(\n","    dataframe=test_df,\n","    x_col='image_path',\n","    y_col='main_class',\n","    class_mode='categorical',\n","    target_size=(IMG_SIZE, IMG_SIZE),\n","    batch_size=output_block1_batch_size,\n","    shuffle=False  # Important: Do not shuffle for evaluation\n",")\n","\n","main_class_predictions = model_main.predict(output_block1_test_generator)\n","predicted_main_classes = np.argmax(main_class_predictions, axis=1)\n","\n","# Preprocess images based on predicted main class\n","for idx, main_class in enumerate(main_classes):\n","    # Filter data for the current main_class\n","    sub_block2_test = test_df[test_df['main_class'] == main_class]\n","\n","    # Preprocess images here based on the predicted main class (e.g., resize, normalization)\n","\n","    # Create a new input layer for your custom sub_class classification layers\n","    inputs_sub = Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n","\n","    # Connect your input to the base VGG16 model\n","    x_sub = base_model(inputs_sub)\n","    x_sub = Flatten()(x_sub)\n","    x_sub = Dense(256, activation='relu')(x_sub)\n","\n","    # Sub class classification\n","    sub_class_output = Dense(sub_block2_test['sub_class'].nunique(), activation='softmax', name='output_sub')(x_sub)\n","\n","    model_sub = Model(inputs=inputs_sub, outputs=[sub_class_output])\n","\n","    model_sub.compile(optimizer=Adam(lr=0.0001),\n","                      loss={'output_sub': 'categorical_crossentropy'},\n","                      metrics={'output_sub': 'accuracy'})\n","\n","    # Create data generator for sub_class prediction\n","    datagen_sub = ImageDataGenerator(rescale=1.0 / 255.0)\n","\n","    output_block2_batch_size = 512\n","\n","    output_block2_test_generator = datagen_sub.flow_from_dataframe(\n","        dataframe=sub_block2_test,\n","        x_col='image_path',\n","        y_col='sub_class',\n","        class_mode='categorical',\n","        target_size=(IMG_SIZE, IMG_SIZE),\n","        batch_size=output_block2_batch_size,\n","        shuffle=False  # Important: Do not shuffle for evaluation\n","    )\n","\n","    # Train the sub_class prediction model for the current main_class\n","    model_sub.fit(\n","        output_block2_test_generator,\n","        epochs=10,  # Adjust the number of epochs\n","        validation_data=output_block2_val_generator\n","    )\n","\n","    sub_models[main_class] = model_sub\n","\n","# Now, you can use sub_models dictionary to make subclass predictions for each main class\n","\n","# Evaluate on the test dataset\n","main_class_eval = model_main.evaluate(output_block1_test_generator)\n","\n","print(\"Main Class Evaluation - Loss:\", main_class_eval[0], \"Accuracy:\", main_class_eval[1])\n","\n","subclass_evaluations = {}\n","\n","for main_class in main_classes:\n","    # Filter data for the current main_class\n","    sub_block2_test = test_df[test_df['main_class'] == main_class]\n","\n","    output_block2_test_generator = datagen_sub.flow_from_dataframe(\n","        dataframe=sub_block2_test,\n","        x_col='image_path',\n","        y_col='sub_class',\n","        class_mode='categorical',\n","        target_size=(IMG_SIZE, IMG_SIZE),\n","        batch_size=output_block2_batch_size,\n","        shuffle=False  # Important: Do not shuffle for evaluation\n","    )\n","\n","    subclass_eval = sub_models[main_class].evaluate(output_block2_test_generator)\n","    subclass_evaluations[main_class] = subclass_eval\n","\n","    print(f\"Subclass Evaluation for Main Class {main_class} - Loss:\", subclass_eval[0], \"Accuracy:\", subclass_eval[1])\n"],"metadata":{"id":"tzC0g8czcLDI"},"execution_count":null,"outputs":[]}]}